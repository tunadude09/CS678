{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# from tensorflow import Tensor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(mnist))\n",
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  validation will be the labaled instances.... train will be the larger number of unlabaled instances\n",
    "#  test will be the test instances in both cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 784)\n",
      "(55000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(mnist.validation.images.shape)\n",
    "print(mnist.train.images.shape)\n",
    "print(mnist.test.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keep_n_labels(passed_data, n):\n",
    "    data = np.copy(passed_data)\n",
    "    all_indexes = np.asarray(list(range(len(data))))\n",
    "    np.random.shuffle(all_indexes)\n",
    "    if n >= 1:\n",
    "        selected_indexes_to_set_to_0 = np.asarray(list(all_indexes[:-n]))\n",
    "    else:\n",
    "        selected_indexes_to_set_to_0 = np.asarray(list(all_indexes[:]))\n",
    "#     normal_indexes = np.asarray(list(all_indexes[-n:]))\n",
    "    data[selected_indexes_to_set_to_0] = np.zeros(data.shape[1])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "50.0\n",
      "500.0\n",
      "10.0\n",
      "10000.0\n",
      "55000.0\n"
     ]
    }
   ],
   "source": [
    "mnist_train_y_0_labeled = keep_n_labels(mnist.train.labels, 0)\n",
    "mnist_train_y_10_labeled = keep_n_labels(mnist.train.labels, 10)\n",
    "mnist_train_y_50_labeled = keep_n_labels(mnist.train.labels, 50)\n",
    "mnist_train_y_100_labeled = keep_n_labels(mnist.train.labels, 100)\n",
    "mnist_train_y_200_labeled = keep_n_labels(mnist.train.labels, 200)\n",
    "mnist_train_y_500_labeled = keep_n_labels(mnist.train.labels, 500)\n",
    "mnist_train_y_1000_labeled = keep_n_labels(mnist.train.labels, 1000)\n",
    "mnist_train_y_5000_labeled = keep_n_labels(mnist.train.labels, 5000)\n",
    "mnist_train_y_10000_labeled = keep_n_labels(mnist.train.labels, 10000)\n",
    "mnist_train_y_all_labeled = np.copy(mnist.train.labels)\n",
    "\n",
    "\n",
    "\n",
    "print(np.sum(mnist_train_y_0_labeled))\n",
    "print(np.sum(mnist_train_y_50_labeled))\n",
    "print(np.sum(mnist_train_y_500_labeled))\n",
    "print(np.sum(mnist_train_y_10_labeled))\n",
    "print(np.sum(mnist_train_y_10000_labeled))\n",
    "print(np.sum(mnist_train_y_all_labeled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Define get next batch method\"\"\"\n",
    "current_index = 0\n",
    "current_data_shuffle_mat = None\n",
    "def get_next_batch(data, classes, num_to_select):\n",
    "    global current_index, current_data_shuffle_mat\n",
    "    #  shuffle with labels\n",
    "    if current_data_shuffle_mat is None or current_index + num_to_select >= len(data):\n",
    "        current_data_shuffle_mat =  np.asarray(zip(data, classes))\n",
    "        np.random.shuffle(current_data_shuffle_mat)\n",
    "        current_index = 0\n",
    "    \n",
    "    data_mat_selected = np.asarray(list(\\\n",
    "                        current_data_shuffle_mat[current_index: current_index + num_to_select,0]))\n",
    "    data_classes_selected = np.asarray(list(\\\n",
    "                        current_data_shuffle_mat[current_index: current_index + num_to_select,1]))\n",
    "    current_index += num_to_select\n",
    "    \n",
    "#     return data_mat_selected.astype(float), data_classes_selected.astype(float)\n",
    "    return data_mat_selected, data_classes_selected\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Import / Clean Sleep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_5_mins = True\n",
    "\n",
    "train_original_df = pd.read_csv('/***')\n",
    "#  choose validation set or test set\n",
    "valid_original_df = pd.read_csv('/***')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10127 entries, 0 to 10126\n",
      "Columns: 609 entries, time to userId\n",
      "dtypes: float64(607), int64(1), object(1)\n",
      "memory usage: 47.1+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>hr0</th>\n",
       "      <th>hr1</th>\n",
       "      <th>hr2</th>\n",
       "      <th>hr3</th>\n",
       "      <th>hr4</th>\n",
       "      <th>hr5</th>\n",
       "      <th>hr6</th>\n",
       "      <th>hr7</th>\n",
       "      <th>hr8</th>\n",
       "      <th>...</th>\n",
       "      <th>ox298</th>\n",
       "      <th>ox299</th>\n",
       "      <th>Invalid</th>\n",
       "      <th>S4</th>\n",
       "      <th>S3</th>\n",
       "      <th>S2</th>\n",
       "      <th>S1</th>\n",
       "      <th>REM</th>\n",
       "      <th>W</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>84.013123</td>\n",
       "      <td>83.012131</td>\n",
       "      <td>82.011139</td>\n",
       "      <td>80.009155</td>\n",
       "      <td>80.009155</td>\n",
       "      <td>81.010147</td>\n",
       "      <td>82.011139</td>\n",
       "      <td>82.011139</td>\n",
       "      <td>83.012131</td>\n",
       "      <td>...</td>\n",
       "      <td>97.001602</td>\n",
       "      <td>98.002594</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>ins2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>69.010452</td>\n",
       "      <td>70.011444</td>\n",
       "      <td>70.011444</td>\n",
       "      <td>70.011444</td>\n",
       "      <td>70.011444</td>\n",
       "      <td>70.011444</td>\n",
       "      <td>69.010452</td>\n",
       "      <td>70.011444</td>\n",
       "      <td>69.010452</td>\n",
       "      <td>...</td>\n",
       "      <td>97.001602</td>\n",
       "      <td>97.001602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>ins2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>77.006180</td>\n",
       "      <td>76.005188</td>\n",
       "      <td>77.006180</td>\n",
       "      <td>76.005188</td>\n",
       "      <td>77.006180</td>\n",
       "      <td>77.006180</td>\n",
       "      <td>77.006180</td>\n",
       "      <td>77.006180</td>\n",
       "      <td>78.007172</td>\n",
       "      <td>...</td>\n",
       "      <td>97.001602</td>\n",
       "      <td>97.001602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>ins2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>75.016403</td>\n",
       "      <td>75.016403</td>\n",
       "      <td>75.016403</td>\n",
       "      <td>76.005188</td>\n",
       "      <td>75.016403</td>\n",
       "      <td>75.016403</td>\n",
       "      <td>75.016403</td>\n",
       "      <td>75.016403</td>\n",
       "      <td>76.005188</td>\n",
       "      <td>...</td>\n",
       "      <td>97.001602</td>\n",
       "      <td>97.001602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>ins2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>72.013428</td>\n",
       "      <td>72.013428</td>\n",
       "      <td>73.014420</td>\n",
       "      <td>72.013428</td>\n",
       "      <td>73.014420</td>\n",
       "      <td>72.013428</td>\n",
       "      <td>73.014420</td>\n",
       "      <td>73.014420</td>\n",
       "      <td>72.013428</td>\n",
       "      <td>...</td>\n",
       "      <td>97.001602</td>\n",
       "      <td>97.001602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>ins2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 609 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   time        hr0        hr1        hr2        hr3        hr4        hr5  \\\n",
       "0     0  84.013123  83.012131  82.011139  80.009155  80.009155  81.010147   \n",
       "1     1  69.010452  70.011444  70.011444  70.011444  70.011444  70.011444   \n",
       "2     2  77.006180  76.005188  77.006180  76.005188  77.006180  77.006180   \n",
       "3     3  75.016403  75.016403  75.016403  76.005188  75.016403  75.016403   \n",
       "4     4  72.013428  72.013428  73.014420  72.013428  73.014420  72.013428   \n",
       "\n",
       "         hr6        hr7        hr8   ...        ox298      ox299   Invalid  \\\n",
       "0  82.011139  82.011139  83.012131   ...    97.001602  98.002594  0.003333   \n",
       "1  69.010452  70.011444  69.010452   ...    97.001602  97.001602  0.000000   \n",
       "2  77.006180  77.006180  78.007172   ...    97.001602  97.001602  0.000000   \n",
       "3  75.016403  75.016403  76.005188   ...    97.001602  97.001602  0.000000   \n",
       "4  73.014420  73.014420  72.013428   ...    97.001602  97.001602  0.000000   \n",
       "\n",
       "    S4   S3   S2   S1  REM         W  userId  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.996667    ins2  \n",
       "1  0.0  0.0  0.0  0.0  0.0  1.000000    ins2  \n",
       "2  0.0  0.0  0.0  0.0  0.0  1.000000    ins2  \n",
       "3  0.0  0.0  0.0  0.0  0.0  1.000000    ins2  \n",
       "4  0.0  0.0  0.0  0.0  0.0  1.000000    ins2  \n",
       "\n",
       "[5 rows x 609 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_original_df.info())\n",
    "train_original_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1253 entries, 0 to 1252\n",
      "Columns: 609 entries, time to userId\n",
      "dtypes: float64(607), int64(1), object(1)\n",
      "memory usage: 5.8+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>hr0</th>\n",
       "      <th>hr1</th>\n",
       "      <th>hr2</th>\n",
       "      <th>hr3</th>\n",
       "      <th>hr4</th>\n",
       "      <th>hr5</th>\n",
       "      <th>hr6</th>\n",
       "      <th>hr7</th>\n",
       "      <th>hr8</th>\n",
       "      <th>...</th>\n",
       "      <th>ox298</th>\n",
       "      <th>ox299</th>\n",
       "      <th>Invalid</th>\n",
       "      <th>S4</th>\n",
       "      <th>S3</th>\n",
       "      <th>S2</th>\n",
       "      <th>S1</th>\n",
       "      <th>REM</th>\n",
       "      <th>W</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>83.076219</td>\n",
       "      <td>82.075227</td>\n",
       "      <td>82.075227</td>\n",
       "      <td>81.074235</td>\n",
       "      <td>81.074235</td>\n",
       "      <td>81.074235</td>\n",
       "      <td>81.074235</td>\n",
       "      <td>81.074235</td>\n",
       "      <td>81.074235</td>\n",
       "      <td>...</td>\n",
       "      <td>95.017929</td>\n",
       "      <td>95.017929</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ins8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>88.068971</td>\n",
       "      <td>88.068971</td>\n",
       "      <td>88.068971</td>\n",
       "      <td>88.068971</td>\n",
       "      <td>87.080186</td>\n",
       "      <td>86.079194</td>\n",
       "      <td>83.076219</td>\n",
       "      <td>81.074235</td>\n",
       "      <td>80.073243</td>\n",
       "      <td>...</td>\n",
       "      <td>94.018463</td>\n",
       "      <td>94.018463</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ins8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>75.083543</td>\n",
       "      <td>75.083543</td>\n",
       "      <td>75.083543</td>\n",
       "      <td>75.083543</td>\n",
       "      <td>74.082551</td>\n",
       "      <td>74.082551</td>\n",
       "      <td>74.082551</td>\n",
       "      <td>73.081559</td>\n",
       "      <td>73.081559</td>\n",
       "      <td>...</td>\n",
       "      <td>93.023575</td>\n",
       "      <td>93.023575</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ins8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>73.081559</td>\n",
       "      <td>74.082551</td>\n",
       "      <td>74.082551</td>\n",
       "      <td>75.083543</td>\n",
       "      <td>75.083543</td>\n",
       "      <td>75.083543</td>\n",
       "      <td>75.083543</td>\n",
       "      <td>74.082551</td>\n",
       "      <td>73.081559</td>\n",
       "      <td>...</td>\n",
       "      <td>93.023575</td>\n",
       "      <td>94.018463</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ins8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>78.071260</td>\n",
       "      <td>78.071260</td>\n",
       "      <td>79.072251</td>\n",
       "      <td>79.072251</td>\n",
       "      <td>79.072251</td>\n",
       "      <td>79.072251</td>\n",
       "      <td>79.072251</td>\n",
       "      <td>79.072251</td>\n",
       "      <td>80.073243</td>\n",
       "      <td>...</td>\n",
       "      <td>95.017929</td>\n",
       "      <td>95.017929</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ins8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 609 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   time        hr0        hr1        hr2        hr3        hr4        hr5  \\\n",
       "0     0  83.076219  82.075227  82.075227  81.074235  81.074235  81.074235   \n",
       "1     1  88.068971  88.068971  88.068971  88.068971  87.080186  86.079194   \n",
       "2     2  75.083543  75.083543  75.083543  75.083543  74.082551  74.082551   \n",
       "3     3  73.081559  74.082551  74.082551  75.083543  75.083543  75.083543   \n",
       "4     4  78.071260  78.071260  79.072251  79.072251  79.072251  79.072251   \n",
       "\n",
       "         hr6        hr7        hr8   ...        ox298      ox299  Invalid  \\\n",
       "0  81.074235  81.074235  81.074235   ...    95.017929  95.017929      1.0   \n",
       "1  83.076219  81.074235  80.073243   ...    94.018463  94.018463      1.0   \n",
       "2  74.082551  73.081559  73.081559   ...    93.023575  93.023575      1.0   \n",
       "3  75.083543  74.082551  73.081559   ...    93.023575  94.018463      1.0   \n",
       "4  79.072251  79.072251  80.073243   ...    95.017929  95.017929      1.0   \n",
       "\n",
       "    S4   S3   S2   S1  REM    W  userId  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0    ins8  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0    ins8  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0    ins8  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0    ins8  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0    ins8  \n",
       "\n",
       "[5 rows x 609 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(valid_original_df.info())\n",
    "valid_original_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_center_data(passed_df):\n",
    "    \"\"\"Takes a train/test/validation set in format time, hr, ox, classes, userId and mean centered the\n",
    "    data for each user\"\"\"\n",
    "    \n",
    "    #  find mean ox and hr\n",
    "    mean_values_by_userId = []\n",
    "\n",
    "    for name, group in passed_df.groupby('userId'):  \n",
    "        if is_5_mins:\n",
    "            mean_hr = group.loc[:, 'hr0':'hr299'].as_matrix().mean()\n",
    "            mean_ox = group.loc[:, 'ox0':'ox299'].as_matrix().mean()\n",
    "        else:\n",
    "            mean_hr = group.loc[:, 'hr0':'hr59'].as_matrix().mean()\n",
    "            mean_ox = group.loc[:, 'ox0':'ox59'].as_matrix().mean()\n",
    "\n",
    "        mean_values_by_userId.append([name, mean_hr, mean_ox])\n",
    "        \n",
    "        \n",
    "    #  convert to dataframe\n",
    "    mean_values_by_userId = pd.DataFrame(mean_values_by_userId)\n",
    "    mean_values_by_userId.columns = ['userId', 'mean_hr', 'mean_ox']\n",
    "        \n",
    "        \n",
    "    #  merge with training/etc data, then mean center, then drop the mean values columns\n",
    "    mean_centered_df = pd.merge(passed_df, mean_values_by_userId, on='userId', how='left')\n",
    "    if is_5_mins:\n",
    "        mean_centered_df.loc[:, 'hr0':'hr299'] = mean_centered_df.loc[:, 'hr0':'hr299'].as_matrix() - \\\n",
    "                                                np.atleast_2d(mean_centered_df.loc[:, 'mean_hr'].as_matrix()).T\n",
    "        mean_centered_df.loc[:, 'ox0':'ox299'] = mean_centered_df.loc[:, 'ox0':'ox299'].as_matrix() - \\\n",
    "                                                np.atleast_2d(mean_centered_df.loc[:, 'mean_ox'].as_matrix()).T\n",
    "    else:\n",
    "        mean_centered_df.loc[:, 'hr0':'hr59'] = mean_centered_df.loc[:, 'hr0':'hr59'].as_matrix() - \\\n",
    "                                                np.atleast_2d(mean_centered_df.loc[:, 'mean_hr'].as_matrix()).T\n",
    "        mean_centered_df.loc[:, 'ox0':'ox59'] = mean_centered_df.loc[:, 'ox0':'ox59'].as_matrix() - \\\n",
    "                                                np.atleast_2d(mean_centered_df.loc[:, 'mean_ox'].as_matrix()).T\n",
    "\n",
    "\n",
    "    mean_centered_df.drop(['mean_hr','mean_ox'], axis=1, inplace=True)\n",
    "    return mean_centered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_centered_train_df = mean_center_data(train_original_df)\n",
    "mean_centered_valid_df = mean_center_data(valid_original_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>hr0</th>\n",
       "      <th>hr1</th>\n",
       "      <th>hr2</th>\n",
       "      <th>hr3</th>\n",
       "      <th>hr4</th>\n",
       "      <th>hr5</th>\n",
       "      <th>hr6</th>\n",
       "      <th>hr7</th>\n",
       "      <th>hr8</th>\n",
       "      <th>...</th>\n",
       "      <th>ox298</th>\n",
       "      <th>ox299</th>\n",
       "      <th>Invalid</th>\n",
       "      <th>S4</th>\n",
       "      <th>S3</th>\n",
       "      <th>S2</th>\n",
       "      <th>S1</th>\n",
       "      <th>REM</th>\n",
       "      <th>W</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18.389260</td>\n",
       "      <td>17.388268</td>\n",
       "      <td>17.388268</td>\n",
       "      <td>16.387276</td>\n",
       "      <td>16.387276</td>\n",
       "      <td>16.387276</td>\n",
       "      <td>16.387276</td>\n",
       "      <td>16.387276</td>\n",
       "      <td>16.387276</td>\n",
       "      <td>...</td>\n",
       "      <td>2.978348</td>\n",
       "      <td>2.978348</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ins8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>23.382012</td>\n",
       "      <td>23.382012</td>\n",
       "      <td>23.382012</td>\n",
       "      <td>23.382012</td>\n",
       "      <td>22.393227</td>\n",
       "      <td>21.392235</td>\n",
       "      <td>18.389260</td>\n",
       "      <td>16.387276</td>\n",
       "      <td>15.386284</td>\n",
       "      <td>...</td>\n",
       "      <td>1.978882</td>\n",
       "      <td>1.978882</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ins8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 609 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   time        hr0        hr1        hr2        hr3        hr4        hr5  \\\n",
       "0     0  18.389260  17.388268  17.388268  16.387276  16.387276  16.387276   \n",
       "1     1  23.382012  23.382012  23.382012  23.382012  22.393227  21.392235   \n",
       "\n",
       "         hr6        hr7        hr8   ...       ox298     ox299  Invalid   S4  \\\n",
       "0  16.387276  16.387276  16.387276   ...    2.978348  2.978348      1.0  0.0   \n",
       "1  18.389260  16.387276  15.386284   ...    1.978882  1.978882      1.0  0.0   \n",
       "\n",
       "    S3   S2   S1  REM    W  userId  \n",
       "0  0.0  0.0  0.0  0.0  0.0    ins8  \n",
       "1  0.0  0.0  0.0  0.0  0.0    ins8  \n",
       "\n",
       "[2 rows x 609 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_centered_valid_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  find majority class labels\n",
    "\n",
    "\n",
    "#  remove class percent columns (6) at the end\n",
    "\n",
    "\n",
    "#  this section accounts for invalid entries. First it actively saves all valid classes as\n",
    "#  last_valid_class, then if any valid classes are encountered they are replaced with this \n",
    "#  last valid class value\n",
    "#  all users are linked together so values might be taken from the user before if they start out invalid\n",
    "#  this should be fairly infrequent though and to boot most users start and end in the wake state so it\n",
    "#  should be alright to link their values back to back like this\n",
    "\n",
    "\n",
    "\n",
    "last_valid_class = 0  #  this is the wake class, it's a fair to assume most start in the wake phase\n",
    "\n",
    "#  majority classes for train\n",
    "majority_classes_train = []\n",
    "for row in mean_centered_train_df.iloc[:,-8:-1].as_matrix():\n",
    "    #  4 classes DEEP(S4/S3), LIGHT(S2/S1), REM, W\n",
    "#     row = [row[0], row[1] + row[2], row[3] + row[4], row[5], row[6]]\n",
    "    #  3 classes DEEP, LIGHT (including REM), Wake\n",
    "    row = [row[0], row[1] + row[2], row[3] + row[4] + row[5], row[6]]\n",
    "    #  2 classes SLEEP, Wake\n",
    "#     row = [row[0], row[1] + row[2] + row[3] + row[4] + row[5], row[6]]\n",
    "\n",
    "\n",
    "    current_class = np.argmax(row) - 1\n",
    "    if current_class < 0:\n",
    "        current_class = last_valid_class\n",
    "    else:\n",
    "        last_valid_class = current_class\n",
    "    majority_classes_train.append(current_class)\n",
    "\n",
    "\n",
    "\n",
    "last_valid_class = 0  #  this is the wake class, it's a fair to assume most start in the wake phase\n",
    "\n",
    "#  majority classes for train\n",
    "majority_classes_valid = []\n",
    "for row in mean_centered_valid_df.iloc[:,-8:-1].as_matrix():\n",
    "    #  4 classes DEEP(S4/S3), LIGHT(S2/S1), REM, W\n",
    "#     row = [row[0], row[1] + row[2], row[3] + row[4], row[5], row[6]]\n",
    "    # 3 classes DEEP, LIGHT (including REM), Wake\n",
    "    row = [row[0], row[1] + row[2], row[3] + row[4] + row[5], row[6]]\n",
    "    #  2 classes SLEEP, Wake\n",
    "#     row = [row[0], row[1] + row[2] + row[3] + row[4] + row[5], row[6]]\n",
    "\n",
    "    \n",
    "    current_class = np.argmax(row) - 1\n",
    "    if current_class < 0:\n",
    "        current_class = last_valid_class\n",
    "    else:\n",
    "        last_valid_class = current_class\n",
    "    majority_classes_valid.append(current_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1800, 6279, 2048])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  check to make sure conversion worked (should see 3 for d,l,w etc)\n",
    "np.bincount(majority_classes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,  13.26134914,  12.26035714, ...,   0.79259893,\n",
       "          0.79259893,   1.79359093],\n",
       "       [  1.        ,  -1.74132186,  -0.74032986, ...,   0.79259893,\n",
       "          0.79259893,   0.79259893],\n",
       "       [  2.        ,   6.25440614,   5.25341414, ...,   0.79259893,\n",
       "          0.79259893,   0.79259893],\n",
       "       ..., \n",
       "       [ 93.        ,  10.38672983,  10.38672983, ...,   1.5138803 ,\n",
       "          1.5138803 ,   1.5138803 ],\n",
       "       [ 94.        ,   1.54526583,   0.63063183, ...,   0.6602223 ,\n",
       "          0.6602223 ,   0.6602223 ],\n",
       "       [ 95.        ,   9.47209483,  10.08185083, ...,   0.5382703 ,\n",
       "          0.6602223 ,   0.6602223 ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  extract stardize shuffle\n",
    "\n",
    "#  extract data for train/valid\n",
    "\n",
    "#  include time (in n minute instances) since sleep began\n",
    "if is_5_mins:\n",
    "    train_original_mat = mean_centered_train_df.loc[:,'time':'ox299'].as_matrix()\n",
    "    valid_original_mat = mean_centered_valid_df.loc[:,'time':'ox299'].as_matrix()\n",
    "else:\n",
    "    train_original_mat = mean_centered_train_df.loc[:,'time':'ox59'].as_matrix()\n",
    "    valid_original_mat = mean_centered_valid_df.loc[:,'time':'ox59'].as_matrix()\n",
    "# train_original_mat = mean_centered_train_df.loc[:,'hr0':'ox299'].as_matrix()\n",
    "# valid_original_mat = mean_centered_valid_df.loc[:,'hr0':'ox299'].as_matrix()\n",
    "\n",
    "train_original_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  standardize\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_original_mat)\n",
    "train_std_mat = scaler.transform(train_original_mat)\n",
    "valid_std_mat = scaler.transform(valid_original_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  shuffle with labels\n",
    "train_shuffled_mat =  np.asarray(zip(train_std_mat, majority_classes_train))\n",
    "np.random.shuffle(train_shuffled_mat)\n",
    "valid_shuffled_mat =  np.asarray(zip(valid_std_mat, majority_classes_valid))\n",
    "np.random.shuffle(valid_shuffled_mat)\n",
    "\n",
    "train_mat = np.asarray(list(train_shuffled_mat[:,0]))\n",
    "train_classes = train_shuffled_mat[:,1]\n",
    "valid_mat = np.asarray(list(valid_shuffled_mat[:,0]))\n",
    "valid_classes = valid_shuffled_mat[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  one hot encoded classes in case I need them\n",
    "\n",
    "#  TODO:  change this for OHE 6 , 4, then 3, 2\n",
    "\n",
    "train_classes_ohe = np.zeros((len(train_classes),3))\n",
    "for i,c in enumerate(train_classes):\n",
    "    train_classes_ohe[i,c] = 1.0\n",
    "    \n",
    "valid_classes_ohe = np.zeros((len(valid_classes),3))\n",
    "for i,c in enumerate(valid_classes):\n",
    "    valid_classes_ohe[i,c] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       ..., \n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classes_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10127, 601)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_std_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "50.0\n",
      "500.0\n",
      "10.0\n",
      "10000.0\n",
      "10127.0\n"
     ]
    }
   ],
   "source": [
    "sleep_train_y_0_labeled = keep_n_labels(train_classes_ohe, 0)\n",
    "sleep_train_y_10_labeled = keep_n_labels(train_classes_ohe, 10)\n",
    "sleep_train_y_50_labeled = keep_n_labels(train_classes_ohe, 50)\n",
    "sleep_train_y_100_labeled = keep_n_labels(train_classes_ohe, 100)\n",
    "sleep_train_y_200_labeled = keep_n_labels(train_classes_ohe, 200)\n",
    "sleep_train_y_500_labeled = keep_n_labels(train_classes_ohe, 500)\n",
    "sleep_train_y_1000_labeled = keep_n_labels(train_classes_ohe, 1000)\n",
    "sleep_train_y_5000_labeled = keep_n_labels(train_classes_ohe, 5000)\n",
    "sleep_train_y_10000_labeled = keep_n_labels(train_classes_ohe, 10000)\n",
    "sleep_train_y_all_labeled = np.copy(train_classes_ohe)\n",
    "\n",
    "\n",
    "\n",
    "print(np.sum(sleep_train_y_0_labeled))\n",
    "print(np.sum(sleep_train_y_50_labeled))\n",
    "print(np.sum(sleep_train_y_500_labeled))\n",
    "print(np.sum(sleep_train_y_10_labeled))\n",
    "print(np.sum(sleep_train_y_10000_labeled))\n",
    "print(np.sum(sleep_train_y_all_labeled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model SLEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train_loss 599.511\n",
      "step 0, validation accuracy 0.260974, loss 711.043\n",
      "step 100, train_loss 65.1258\n",
      "step 100, validation accuracy 0.177973, loss 706.467\n",
      "step 200, train_loss 94.8783\n",
      "step 200, validation accuracy 0.178771, loss 706.178\n",
      "step 300, train_loss 209.753\n",
      "step 300, validation accuracy 0.190742, loss 706.343\n",
      "step 400, train_loss 59.3607\n",
      "step 400, validation accuracy 0.189146, loss 706.424\n",
      "step 500, train_loss 173.361\n",
      "step 500, validation accuracy 0.185954, loss 706.61\n",
      "step 600, train_loss 57.2304\n",
      "step 600, validation accuracy 0.186752, loss 706.88\n",
      "step 700, train_loss 162.288\n",
      "step 700, validation accuracy 0.18755, loss 707.088\n",
      "step 800, train_loss 212.252\n",
      "step 800, validation accuracy 0.18755, loss 707.345\n",
      "step 900, train_loss 203.412\n",
      "step 900, validation accuracy 0.185954, loss 707.512\n",
      "step 1000, train_loss 83.6336\n",
      "step 1000, validation accuracy 0.186752, loss 707.905\n",
      "step 1100, train_loss 482.394\n",
      "step 1100, validation accuracy 0.185954, loss 708.095\n",
      "step 1200, train_loss 106.116\n",
      "step 1200, validation accuracy 0.181963, loss 708.28\n",
      "step 1300, train_loss 90.5836\n",
      "step 1300, validation accuracy 0.161213, loss 710.013\n",
      "step 1400, train_loss 50.244\n",
      "step 1400, validation accuracy 0.163607, loss 710.002\n",
      "step 1500, train_loss 164.48\n",
      "step 1500, validation accuracy 0.165204, loss 710.029\n",
      "step 1600, train_loss 115.333\n",
      "step 1600, validation accuracy 0.161213, loss 710.115\n",
      "step 1700, train_loss 231.872\n",
      "step 1700, validation accuracy 0.164405, loss 710.328\n",
      "step 1800, train_loss 93.9676\n",
      "step 1800, validation accuracy 0.166002, loss 710.126\n",
      "step 1900, train_loss 100.078\n",
      "step 1900, validation accuracy 0.161213, loss 710.128\n",
      "step 2000, train_loss 236.677\n",
      "step 2000, validation accuracy 0.158021, loss 711.094\n",
      "step 2100, train_loss 92.2798\n",
      "step 2100, validation accuracy 0.152434, loss 710.717\n",
      "step 2200, train_loss 117.011\n",
      "step 2200, validation accuracy 0.155626, loss 711.269\n",
      "step 2300, train_loss 77.1758\n",
      "step 2300, validation accuracy 0.145251, loss 711.155\n",
      "step 2400, train_loss 74.2615\n",
      "step 2400, validation accuracy 0.151636, loss 712.11\n",
      "step 2500, train_loss 78.9217\n",
      "step 2500, validation accuracy 0.179569, loss 712.516\n",
      "step 2600, train_loss 54.3854\n",
      "step 2600, validation accuracy 0.284118, loss 712.333\n",
      "step 2700, train_loss 115.805\n",
      "step 2700, validation accuracy 0.289705, loss 712.665\n",
      "step 2800, train_loss 120.656\n",
      "step 2800, validation accuracy 0.252993, loss 713.091\n",
      "step 2900, train_loss 53.2407\n",
      "step 2900, validation accuracy 0.288109, loss 714.46\n",
      "step 3000, train_loss 122.087\n",
      "step 3000, validation accuracy 0.310455, loss 714.628\n",
      "step 3100, train_loss 128.301\n",
      "step 3100, validation accuracy 0.277733, loss 715.904\n",
      "step 3200, train_loss 142.827\n",
      "step 3200, validation accuracy 0.285714, loss 723.382\n",
      "step 3300, train_loss 45.5755\n",
      "step 3300, validation accuracy 0.288907, loss 721.6\n",
      "step 3400, train_loss 124.999\n",
      "step 3400, validation accuracy 0.292897, loss 721.204\n",
      "step 3500, train_loss 78.8504\n",
      "step 3500, validation accuracy 0.32482, loss 717.523\n",
      "step 3600, train_loss 106.437\n",
      "step 3600, validation accuracy 0.296089, loss 719.455\n",
      "step 3700, train_loss 101.145\n",
      "step 3700, validation accuracy 0.288907, loss 717.882\n",
      "step 3800, train_loss 93.3168\n",
      "step 3800, validation accuracy 0.320032, loss 720.308\n",
      "step 3900, train_loss 118.719\n",
      "step 3900, validation accuracy 0.329609, loss 717.309\n",
      "step 4000, train_loss 200.905\n",
      "step 4000, validation accuracy 0.339186, loss 718.129\n",
      "step 4100, train_loss 108.129\n",
      "step 4100, validation accuracy 0.367917, loss 719.127\n",
      "step 4200, train_loss 90.5823\n",
      "step 4200, validation accuracy 0.28332, loss 719.613\n",
      "step 4300, train_loss 202.563\n",
      "step 4300, validation accuracy 0.363927, loss 723.235\n",
      "step 4400, train_loss 81.2914\n",
      "step 4400, validation accuracy 0.206704, loss 721.522\n",
      "step 4500, train_loss 382.078\n",
      "step 4500, validation accuracy 0.302474, loss 718.915\n",
      "step 4600, train_loss 45.5147\n",
      "step 4600, validation accuracy 0.193935, loss 723.127\n",
      "step 4700, train_loss 228.815\n",
      "step 4700, validation accuracy 0.214685, loss 722.041\n",
      "step 4800, train_loss 95.2152\n",
      "step 4800, validation accuracy 0.181165, loss 724.698\n",
      "step 4900, train_loss 162.24\n",
      "step 4900, validation accuracy 0.276935, loss 724.614\n",
      "step 5000, train_loss 165.164\n",
      "step 5000, validation accuracy 0.248204, loss 721.41\n",
      "step 5100, train_loss 49.3321\n",
      "step 5100, validation accuracy 0.195531, loss 721.934\n",
      "step 5200, train_loss 63.2056\n",
      "step 5200, validation accuracy 0.181165, loss 721.856\n",
      "step 5300, train_loss 102.102\n",
      "step 5300, validation accuracy 0.173982, loss 718.515\n",
      "step 5400, train_loss 54.0638\n",
      "step 5400, validation accuracy 0.264964, loss 721.646\n",
      "step 5500, train_loss 149.34\n",
      "step 5500, validation accuracy 0.237829, loss 720.249\n",
      "step 5600, train_loss 355.752\n",
      "step 5600, validation accuracy 0.214685, loss 722.58\n",
      "step 5700, train_loss 175.591\n",
      "step 5700, validation accuracy 0.347167, loss 722.183\n",
      "step 5800, train_loss 115.544\n",
      "step 5800, validation accuracy 0.21229, loss 720.337\n",
      "step 5900, train_loss 53.0206\n",
      "step 5900, validation accuracy 0.408619, loss 726.136\n",
      "step 6000, train_loss 109.098\n",
      "step 6000, validation accuracy 0.350359, loss 723.88\n",
      "step 6100, train_loss 87.655\n",
      "step 6100, validation accuracy 0.296887, loss 720.893\n",
      "step 6200, train_loss 503.2\n",
      "step 6200, validation accuracy 0.339984, loss 720.927\n",
      "step 6300, train_loss 47.1435\n",
      "step 6300, validation accuracy 0.19154, loss 722.464\n",
      "step 6400, train_loss 187.561\n",
      "step 6400, validation accuracy 0.1668, loss 720.362\n",
      "step 6500, train_loss 169.976\n",
      "step 6500, validation accuracy 0.255387, loss 733.588\n",
      "step 6600, train_loss 304.711\n",
      "step 6600, validation accuracy 0.288907, loss 723.921\n",
      "step 6700, train_loss 69.8356\n",
      "step 6700, validation accuracy 0.309657, loss 723.246\n",
      "step 6800, train_loss 120.58\n",
      "step 6800, validation accuracy 0.329609, loss 722.153\n",
      "step 6900, train_loss 87.1845\n",
      "step 6900, validation accuracy 0.234637, loss 723.159\n",
      "step 7000, train_loss 105.437\n",
      "step 7000, validation accuracy 0.335994, loss 720.414\n",
      "step 7100, train_loss 155.211\n",
      "step 7100, validation accuracy 0.312849, loss 721.353\n",
      "step 7200, train_loss 203.661\n",
      "step 7200, validation accuracy 0.333599, loss 721.031\n",
      "step 7300, train_loss 128.643\n",
      "step 7300, validation accuracy 0.318436, loss 720.779\n",
      "step 7400, train_loss 321.436\n",
      "step 7400, validation accuracy 0.324022, loss 721.672\n",
      "step 7500, train_loss 398.787\n",
      "step 7500, validation accuracy 0.305666, loss 720.239\n",
      "step 7600, train_loss 53.7785\n",
      "step 7600, validation accuracy 0.158021, loss 721.481\n",
      "step 7700, train_loss 64.7011\n",
      "step 7700, validation accuracy 0.318436, loss 724.244\n",
      "step 7800, train_loss 46.9414\n",
      "step 7800, validation accuracy 0.278532, loss 722.712\n",
      "step 7900, train_loss 79.6704\n",
      "step 7900, validation accuracy 0.304868, loss 721.945\n",
      "step 8000, train_loss 124.112\n",
      "step 8000, validation accuracy 0.371109, loss 721.658\n",
      "step 8100, train_loss 174.701\n",
      "step 8100, validation accuracy 0.329609, loss 720.144\n",
      "step 8200, train_loss 183.384\n",
      "step 8200, validation accuracy 0.32482, loss 720.29\n",
      "step 8300, train_loss 162.35\n",
      "step 8300, validation accuracy 0.334397, loss 720.428\n",
      "step 8400, train_loss 385.415\n",
      "step 8400, validation accuracy 0.321628, loss 718.976\n",
      "step 8500, train_loss 310.083\n",
      "step 8500, validation accuracy 0.331205, loss 720.693\n",
      "step 8600, train_loss 192.693\n",
      "step 8600, validation accuracy 0.327215, loss 719.655\n",
      "step 8700, train_loss 199.408\n",
      "step 8700, validation accuracy 0.327215, loss 719.789\n",
      "step 8800, train_loss 104.416\n",
      "step 8800, validation accuracy 0.33759, loss 720.177\n",
      "step 8900, train_loss 266.623\n",
      "step 8900, validation accuracy 0.349561, loss 718.56\n",
      "step 9000, train_loss 489.837\n",
      "step 9000, validation accuracy 0.330407, loss 718.618\n",
      "step 9100, train_loss 104.791\n",
      "step 9100, validation accuracy 0.321628, loss 718.007\n",
      "step 9200, train_loss 93.4477\n",
      "step 9200, validation accuracy 0.324022, loss 718.868\n",
      "step 9300, train_loss 82.5137\n",
      "step 9300, validation accuracy 0.343176, loss 718.221\n",
      "step 9400, train_loss 149.229\n",
      "step 9400, validation accuracy 0.339984, loss 719.431\n",
      "step 9500, train_loss 75.5746\n",
      "step 9500, validation accuracy 0.347965, loss 720.662\n",
      "step 9600, train_loss 78.751\n",
      "step 9600, validation accuracy 0.347167, loss 720.982\n",
      "step 9700, train_loss 166.784\n",
      "step 9700, validation accuracy 0.359138, loss 721.603\n",
      "step 9800, train_loss 158.523\n",
      "step 9800, validation accuracy 0.346369, loss 720.328\n",
      "step 9900, train_loss 70.2144\n",
      "step 9900, validation accuracy 0.373504, loss 719.352\n",
      "test accuracy 0.347965\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()    \n",
    "   \n",
    "def relu(passed_mat):\n",
    "    return tf.maximum(0.0, passed_mat)\n",
    "\n",
    "def softmax(passed_mat):\n",
    "    return tf.exp(passed_mat) / tf.reduce_sum(tf.exp(passed_mat))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  batch norm without scale or offset\n",
    "def batch_norm(passed_mat, isTrain, isUncorrupted):\n",
    "    e = 0.00001\n",
    "    decay = 0.999\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    #  track this with moving averages in order to make predictions on the fly\n",
    "    pop_mean = tf.Variable(tf.zeros([passed_mat.get_shape()[-1]]), trainable=False)\n",
    "    pop_var = tf.Variable(tf.ones([passed_mat.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "    \n",
    "    \n",
    "#     if isTrain:\n",
    "#     if Tensor.eval(training):\n",
    "    def training_batch_norn():\n",
    "        mean, var = tf.nn.moments(passed_mat, axes=[0])\n",
    "        #  use moving average to estimate population averages on the fly\n",
    "        \n",
    "        if isUncorrupted:\n",
    "            train_mean = tf.assign(pop_mean, pop_mean * decay + mean * (1 - decay))\n",
    "            train_var = tf.assign(pop_var, pop_var * decay + var * (1 - decay))\n",
    "        else:\n",
    "            train_mean = mean\n",
    "            train_var = var\n",
    "        \n",
    "        #  this makes sure that the pop weights are updated before batch_norm executes\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return _batch_norm_op(passed_mat, mean, var, e)\n",
    "\n",
    "    \n",
    "    def predictive_batch_norm():\n",
    "        #  use population mean and variance instead of batch values\n",
    "        return _batch_norm_op(passed_mat, pop_mean, pop_var, e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return tf.cond(isTrain, training_batch_norn, predictive_batch_norm)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _batch_norm_op(passed_mat, mean, var, e):\n",
    "    #  helps prevent dividing by 0\n",
    "    var_scale = tf.rsqrt(var + e)\n",
    "    bn_out = (passed_mat * var_scale) - (mean * var_scale)\n",
    "    return bn_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  input data and targets\n",
    "x = tf.placeholder(tf.float32, [None, 601], name=\"x\")\n",
    "y_ = tf.placeholder(tf.float32, [None, 3], name=\"y_\")\n",
    "isTrain = tf.placeholder(tf.bool, shape=(), name=\"isTrain\")\n",
    "isLabeled = tf.placeholder(tf.bool, shape=(), name=\"isLabeled\")\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "with tf.name_scope('LadderNetwork') as scope:\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.name_scope('UncorruptedForwardPass'):\n",
    "\n",
    "\n",
    "        #  layer 1\n",
    "        with tf.name_scope('Layer1'):\n",
    "            w1_dim_1, w1_dim_2 = 601, 400\n",
    "            W1 = tf.Variable(tf.random_normal([w1_dim_1, w1_dim_2], stddev=0.001))\n",
    "            b1 = tf.Variable(tf.random_normal([w1_dim_2], stddev=0.001))\n",
    "            z1 = batch_norm(tf.matmul(x, W1) + b1, isTrain, True)\n",
    "            #  these will be trained and adjusted with all other variables\n",
    "            scale1 = tf.Variable(tf.ones([z1.get_shape()[-1]]))\n",
    "            beta1 = tf.Variable(tf.zeros([z1.get_shape()[-1]]))\n",
    "            h1 = relu(scale1 * z1 + beta1)\n",
    "\n",
    "\n",
    "        #  layer 2\n",
    "        with tf.name_scope('Layer2'):\n",
    "            w2_dim_1, w2_dim_2 = 400, 200\n",
    "            W2 = tf.Variable(tf.random_normal([w2_dim_1, w2_dim_2], stddev=0.001))\n",
    "            b2 = tf.Variable(tf.random_normal([w2_dim_2], stddev=0.001))\n",
    "            z2 = batch_norm(tf.matmul(h1, W2) + b2, isTrain, True)\n",
    "            #  these will be trained and adjusted with all other variables\n",
    "            scale2 = tf.Variable(tf.ones([z2.get_shape()[-1]]))\n",
    "            beta2 = tf.Variable(tf.zeros([z2.get_shape()[-1]]))\n",
    "            h2 = relu(scale2 * z2 + beta2)\n",
    "\n",
    "\n",
    "        #  layer 3\n",
    "        with tf.name_scope('Layer3'):\n",
    "            w3_dim_1, w3_dim_2 = 200, 3\n",
    "            W3 = tf.Variable(tf.random_normal([w3_dim_1, w3_dim_2], stddev=0.001))\n",
    "            b3 = tf.Variable(tf.random_normal([w3_dim_2], stddev=0.001))\n",
    "            z3 = batch_norm(tf.matmul(h2, W3) + b3, isTrain, True)\n",
    "            #  these will be trained and adjusted with all other variables\n",
    "            scale3 = tf.Variable(tf.ones([z3.get_shape()[-1]]))\n",
    "            beta3 = tf.Variable(tf.zeros([z3.get_shape()[-1]]))\n",
    "            h3 = softmax(scale3 * z3 + beta3)\n",
    "            y = h3\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.name_scope('CorruptedForwardPass'):\n",
    "\n",
    "        #  _n stands for noisy versions in the corrupted forward pass\n",
    "        true_const_tensor = tf.constant(True)\n",
    "        std = 0.2\n",
    "        \n",
    "        \n",
    "        noise0 = tf.random_normal(shape=tf.shape(x), mean=0.0, stddev=std, dtype=tf.float32) \n",
    "        x_n = h0_n = x + noise0\n",
    "        \n",
    "        \n",
    "        #  layer 1\n",
    "        with tf.name_scope('Layer1'):\n",
    "            z1_n_pre = tf.matmul(h0_n, W1) + b1\n",
    "            noise1 = tf.random_normal(shape=tf.shape(z1_n_pre), mean=0.0, stddev=std, dtype=tf.float32) \n",
    "            z1_n = batch_norm(z1_n_pre, true_const_tensor, False) + noise1\n",
    "            h1_n = relu(scale1 * z1 + beta1)\n",
    "\n",
    "\n",
    "        #  layer 2\n",
    "        with tf.name_scope('Layer2'):\n",
    "            z2_n_pre = tf.matmul(h1_n, W2) + b2\n",
    "            noise2 = tf.random_normal(shape=tf.shape(z2_n_pre), mean=0.0, stddev=std, dtype=tf.float32) \n",
    "            z2_n = batch_norm(z2_n_pre, true_const_tensor, False) + noise2\n",
    "            h2_n = relu(scale2 * z2 + beta2)\n",
    "\n",
    "\n",
    "        #  layer 3\n",
    "        with tf.name_scope('Layer3'):\n",
    "            z3_n_pre = tf.matmul(h2_n, W3) + b3\n",
    "            noise3 = tf.random_normal(shape=tf.shape(z3_n_pre), mean=0.0, stddev=std, dtype=tf.float32) \n",
    "            z3_n = batch_norm(z3_n_pre, true_const_tensor, False) + noise3\n",
    "            h3_n = softmax(scale3 * z3 + beta3)\n",
    "            y_n = h3_n\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    with tf.name_scope('DenoisingBackwardsPass'):\n",
    "\n",
    "\n",
    "        def g_gauss(z_c, u, size):\n",
    "            \"gaussian denoising function proposed in the original paper\"\n",
    "            wi = lambda inits, name: tf.Variable(inits * tf.ones([size]), name=name)\n",
    "            a1 = wi(0., 'a1')\n",
    "            a2 = wi(1., 'a2')\n",
    "            a3 = wi(0., 'a3')\n",
    "            a4 = wi(0., 'a4')\n",
    "            a5 = wi(0., 'a5')\n",
    "\n",
    "            a6 = wi(0., 'a6')\n",
    "            a7 = wi(1., 'a7')\n",
    "            a8 = wi(0., 'a8')\n",
    "            a9 = wi(0., 'a9')\n",
    "            a10 = wi(0., 'a10')\n",
    "\n",
    "            mu = a1 * tf.sigmoid(a2 * u + a3) + a4 * u + a5\n",
    "            v = a6 * tf.sigmoid(a7 * u + a8) + a9 * u + a10\n",
    "\n",
    "            z_est = (z_c - mu) * v + mu\n",
    "            return z_est\n",
    "\n",
    "\n",
    "\n",
    "        d_cost = []  # to store the denoising cost of all layers\n",
    "\n",
    "        #  layer 3\n",
    "        with tf.name_scope('Layer3'):\n",
    "            \n",
    "            #  this first layer doesn not downsample but measures the loss of of output as compared\n",
    "            #  to it's noisey version and it's batch normed counter part\n",
    "            z = z3\n",
    "            z_c = z3_n\n",
    "\n",
    "            u = y_n\n",
    "            #  TODO:  put u_bn back to batch norm\n",
    "            u_bn = y_n\n",
    "#             u_bn = batch_norm(u, isTrain, False)\n",
    "            z3_dn = g_gauss(z_c, u_bn, w3_dim_2)\n",
    "            z3_dn_bn = batch_norm(z3_dn, isTrain, False)\n",
    "    \n",
    "            # append the cost of this layer to d_cost\n",
    "            d_cost.append((tf.reduce_mean(tf.reduce_sum(tf.square(z - z3_dn_bn), 1)) / w3_dim_2) * 0.10) \n",
    "    \n",
    "    \n",
    "\n",
    "        #  layer 2\n",
    "        with tf.name_scope('Layer2'):\n",
    "            \n",
    "            V2 = tf.Variable(tf.random_normal([w3_dim_2, w3_dim_1], stddev=0.001))\n",
    "            u_bn = batch_norm(tf.matmul(z3_dn, V2), isTrain, False)  \n",
    "            \n",
    "            z = z2\n",
    "            z_c = z2_n\n",
    "            z2_dn = g_gauss(z_c, u_bn, w2_dim_2)\n",
    "            z2_dn_bn = batch_norm(z2_dn, isTrain, False)\n",
    "\n",
    "            # append the cost of this layer to d_cost\n",
    "            d_cost.append((tf.reduce_mean(tf.reduce_sum(tf.square(z - z2_dn_bn), 1)) / w2_dim_2) * 0.10)\n",
    "           \n",
    "              \n",
    "        \n",
    "        #  layer 1\n",
    "        with tf.name_scope('Layer1'):\n",
    "                \n",
    "            V1 = tf.Variable(tf.random_normal([w2_dim_2, w2_dim_1], stddev=0.001))\n",
    "            u_bn = batch_norm(tf.matmul(z2_dn, V1), isTrain, False) \n",
    "                       \n",
    "            z = z1\n",
    "            z_c = z1_n\n",
    "            z1_dn = g_gauss(z_c, u_bn, w1_dim_2)\n",
    "            z1_dn_bn = batch_norm(z1_dn, isTrain, False)  #  batch norm the denoised z for the d_cost\n",
    "\n",
    "            # append the cost of this layer to d_cost\n",
    "            d_cost.append((tf.reduce_mean(tf.reduce_sum(tf.square(z - z1_dn_bn), 1)) / w1_dim_2) * 10.0)\n",
    "  \n",
    "            \n",
    "                   \n",
    "        #  layer 0\n",
    "        with tf.name_scope('Layer0'): \n",
    "                \n",
    "            V0 = tf.Variable(tf.random_normal([w1_dim_2, w1_dim_1], stddev=0.001))\n",
    "            u_bn = batch_norm(tf.matmul(z1_dn, V0), isTrain, False) \n",
    "                       \n",
    "            z = x\n",
    "            z_c = x_n\n",
    "            z0_dn = g_gauss(z_c, u_bn, w1_dim_1)\n",
    "            z0_dn_bn = batch_norm(z0_dn, isTrain, False)\n",
    "\n",
    "            # append the cost of this layer to d_cost\n",
    "            d_cost.append((tf.reduce_mean(tf.reduce_sum(tf.square(z - z0_dn_bn), 1)) / w1_dim_1) * 1000.0)\n",
    "\n",
    "        x_dn = z0_dn\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope('SupervisedLoss') as scope:\n",
    "    def cross_ent():\n",
    "        return tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_n), reduction_indices=[1]))\n",
    "    def set_to_zero():\n",
    "        return tf.constant(0.0)\n",
    "    cross_entropy = tf.cond(isLabeled, cross_ent, set_to_zero)\n",
    "#     cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_n), reduction_indices=[1]))\n",
    "\n",
    "    \n",
    "\n",
    "with tf.name_scope('UnupervisedLoss') as scope:\n",
    "    # calculate total unsupervised cost by adding the denoising cost of all layers\n",
    "    unsupervised_cost = tf.add_n(d_cost)\n",
    "    loss = cross_entropy + unsupervised_cost  # total cost\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "with tf.name_scope('Optimizer') as scope:\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "#     train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    \n",
    "with tf.name_scope('Accuracy') as scope:\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#  summary writers for debugging\n",
    "summary_writer = tf.summary.FileWriter( \\\n",
    " '/home/tuna/Projects/CS678/Ladder_Networks_Project_1/tf_logs/ln_sleep_100_labels/v1',\\\n",
    "                                    graph=sess.graph )\n",
    "acc_summary = tf.summary.scalar( 'accuracy', accuracy )\n",
    "loss_summary = tf.summary.scalar( 'loss', loss )\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "#  additional for seperate runs\n",
    "valid_acc_summary = tf.summary.scalar( 'validation_accuracy', accuracy )\n",
    "test_acc_summary = tf.summary.scalar( 'test_accuracy', accuracy )\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "\n",
    "for i in range(10000):\n",
    "#     batch = mnist.train.next_batch(batch_size)\n",
    "    batch_xs, batch_ys = get_next_batch(train_mat, sleep_train_y_10_labeled, batch_size)\n",
    "\n",
    "#     if i < 1000:\n",
    "#         _, summary_, train_loss = sess.run([train_step, merged_summary_op, loss], feed_dict={x: batch_xs, y_: batch_ys,\n",
    "#                                                                           isTrain: True, isLabeled: True})\n",
    "#     else:\n",
    "#         _, summary_, train_loss = sess.run([train_step, merged_summary_op, loss], feed_dict={x: batch_xs, y_: np.zeros(batch_ys.shape, dtype=np.float32),\n",
    "#                                                                           isTrain: True, isLabeled: True})\n",
    "#     _, summary_ = sess.run([train_step, merged_summary_op], feed_dict={x: batch[0], y_: np.zeros(batch[1].shape, dtype=np.float32),\n",
    "#                                                                       isTrain: True, isLabeled: False})\n",
    "    _, summary_, train_loss = sess.run([train_step, merged_summary_op, loss], feed_dict={x: batch_xs, y_: batch_ys, isTrain: True, isLabeled: True})\n",
    "    \n",
    "\n",
    "    summary_writer.add_summary(summary_, i)\n",
    "    if i%100 == 0:\n",
    "        #  print training loss\n",
    "        print(\"step %d, train_loss %g\" % (i, train_loss))\n",
    "        \n",
    "        #  validation set results\n",
    "        acc, ls, valid_summary = sess.run([accuracy, loss, valid_acc_summary], \\\n",
    "                                        feed_dict={x: valid_mat, y_: valid_classes_ohe, isTrain: False, isLabeled: True})\n",
    "        summary_writer.add_summary(valid_summary, i)\n",
    "        print(\"step %d, validation accuracy %g, loss %g\" % (i, acc, ls))\n",
    "\n",
    "        \n",
    "#  test set results\n",
    "acc, test_acc_summary = sess.run([accuracy, test_acc_summary], feed_dict={x: valid_mat, y_: valid_classes_ohe, isTrain: False, isLabeled: True})\n",
    "summary_writer.add_summary(test_acc_summary, 1)\n",
    "summary_writer.add_summary(test_acc_summary, 100)\n",
    "summary_writer.add_summary(test_acc_summary, 1000)\n",
    "summary_writer.add_summary(test_acc_summary, 10)\n",
    "print(\"test accuracy %g\" % acc)\n",
    "\n",
    "\n",
    "\n",
    "summary_writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train_loss 127.136\n",
      "step 0, validation accuracy 0.0924, loss 122.314\n",
      "step 100, train_loss 53.0978\n",
      "step 100, validation accuracy 0.11, loss 121.876\n",
      "step 200, train_loss 56.4439\n",
      "step 200, validation accuracy 0.0924, loss 121.899\n",
      "step 300, train_loss 58.4609\n",
      "step 300, validation accuracy 0.0924, loss 121.87\n",
      "step 400, train_loss 48.1426\n",
      "step 400, validation accuracy 0.0924, loss 121.812\n",
      "step 500, train_loss 48.8902\n",
      "step 500, validation accuracy 0.0924, loss 121.755\n",
      "step 600, train_loss 49.6925\n",
      "step 600, validation accuracy 0.0924, loss 121.701\n",
      "step 700, train_loss 52.6471\n",
      "step 700, validation accuracy 0.0924, loss 121.63\n",
      "step 800, train_loss 51.6703\n",
      "step 800, validation accuracy 0.0924, loss 121.559\n",
      "step 900, train_loss 48.8234\n",
      "step 900, validation accuracy 0.0924, loss 121.492\n",
      "step 1000, train_loss 56.299\n",
      "step 1000, validation accuracy 0.0924, loss 121.418\n",
      "step 1100, train_loss 52.5355\n",
      "step 1100, validation accuracy 0.0924, loss 121.353\n",
      "step 1200, train_loss 47.0186\n",
      "step 1200, validation accuracy 0.0924, loss 121.274\n",
      "step 1300, train_loss 50.7475\n",
      "step 1300, validation accuracy 0.0924, loss 121.199\n",
      "step 1400, train_loss 48.1878\n",
      "step 1400, validation accuracy 0.0924, loss 121.119\n",
      "step 1500, train_loss 52.0145\n",
      "step 1500, validation accuracy 0.0924, loss 121.053\n",
      "step 1600, train_loss 50.79\n",
      "step 1600, validation accuracy 0.0924, loss 120.974\n",
      "step 1700, train_loss 47.1109\n",
      "step 1700, validation accuracy 0.0924, loss 120.899\n",
      "step 1800, train_loss 43.0924\n",
      "step 1800, validation accuracy 0.0924, loss 120.834\n",
      "step 1900, train_loss 49.6969\n",
      "step 1900, validation accuracy 0.0924, loss 120.775\n",
      "step 2000, train_loss 53.3512\n",
      "step 2000, validation accuracy 0.0924, loss 120.736\n",
      "step 2100, train_loss 48.4784\n",
      "step 2100, validation accuracy 0.0924, loss 120.702\n",
      "step 2200, train_loss 52.3075\n",
      "step 2200, validation accuracy 0.0924, loss 120.693\n",
      "step 2300, train_loss 60.1802\n",
      "step 2300, validation accuracy 0.0924, loss 120.687\n",
      "step 2400, train_loss 51.0637\n",
      "step 2400, validation accuracy 0.0924, loss 120.715\n",
      "step 2500, train_loss 50.2912\n",
      "step 2500, validation accuracy 0.0924, loss 120.784\n",
      "step 2600, train_loss 48.5075\n",
      "step 2600, validation accuracy 0.0924, loss 120.878\n",
      "step 2700, train_loss 60.1438\n",
      "step 2700, validation accuracy 0.0926, loss 120.986\n",
      "step 2800, train_loss 60.3375\n",
      "step 2800, validation accuracy 0.0976, loss 121.107\n",
      "step 2900, train_loss 53.5699\n",
      "step 2900, validation accuracy 0.1014, loss 121.259\n",
      "step 3000, train_loss 48.6532\n",
      "step 3000, validation accuracy 0.1212, loss 121.419\n",
      "step 3100, train_loss 55.3843\n",
      "step 3100, validation accuracy 0.1326, loss 121.608\n",
      "step 3200, train_loss 56.5058\n",
      "step 3200, validation accuracy 0.1608, loss 121.788\n",
      "step 3300, train_loss 56.5068\n",
      "step 3300, validation accuracy 0.177, loss 121.984\n",
      "step 3400, train_loss 55.9449\n",
      "step 3400, validation accuracy 0.2274, loss 122.135\n",
      "step 3500, train_loss 54.0903\n",
      "step 3500, validation accuracy 0.253, loss 122.334\n",
      "step 3600, train_loss 50.31\n",
      "step 3600, validation accuracy 0.422, loss 122.528\n",
      "step 3700, train_loss 48.1029\n",
      "step 3700, validation accuracy 0.4764, loss 122.722\n",
      "step 3800, train_loss 52.1232\n",
      "step 3800, validation accuracy 0.5516, loss 122.937\n",
      "step 3900, train_loss 54.4575\n",
      "step 3900, validation accuracy 0.6196, loss 123.145\n",
      "step 4000, train_loss 62.812\n",
      "step 4000, validation accuracy 0.6816, loss 123.395\n",
      "step 4100, train_loss 55.3802\n",
      "step 4100, validation accuracy 0.7818, loss 123.616\n",
      "step 4200, train_loss 49.8076\n",
      "step 4200, validation accuracy 0.8036, loss 123.854\n",
      "step 4300, train_loss 51.8667\n",
      "step 4300, validation accuracy 0.834, loss 124.084\n",
      "step 4400, train_loss 52.1127\n",
      "step 4400, validation accuracy 0.8592, loss 124.346\n",
      "step 4500, train_loss 53.3695\n",
      "step 4500, validation accuracy 0.8524, loss 124.549\n",
      "step 4600, train_loss 43.9474\n",
      "step 4600, validation accuracy 0.8838, loss 124.794\n",
      "step 4700, train_loss 45.6992\n",
      "step 4700, validation accuracy 0.892, loss 125.026\n",
      "step 4800, train_loss 53.9453\n",
      "step 4800, validation accuracy 0.9132, loss 125.253\n",
      "step 4900, train_loss 50.7729\n",
      "step 4900, validation accuracy 0.9204, loss 125.465\n",
      "step 5000, train_loss 50.7677\n",
      "step 5000, validation accuracy 0.923, loss 125.692\n",
      "step 5100, train_loss 48.9949\n",
      "step 5100, validation accuracy 0.9214, loss 125.932\n",
      "step 5200, train_loss 45.8868\n",
      "step 5200, validation accuracy 0.9224, loss 126.156\n",
      "step 5300, train_loss 47.3291\n",
      "step 5300, validation accuracy 0.9194, loss 126.34\n",
      "step 5400, train_loss 54.8045\n",
      "step 5400, validation accuracy 0.926, loss 126.513\n",
      "step 5500, train_loss 52.9802\n",
      "step 5500, validation accuracy 0.923, loss 126.651\n",
      "step 5600, train_loss 55.861\n",
      "step 5600, validation accuracy 0.921, loss 126.845\n",
      "step 5700, train_loss 52.4349\n",
      "step 5700, validation accuracy 0.923, loss 126.942\n",
      "step 5800, train_loss 63.4064\n",
      "step 5800, validation accuracy 0.9214, loss 127.132\n",
      "step 5900, train_loss 44.1467\n",
      "step 5900, validation accuracy 0.9222, loss 127.293\n",
      "step 6000, train_loss 49.212\n",
      "step 6000, validation accuracy 0.925, loss 127.345\n",
      "step 6100, train_loss 55.5998\n",
      "step 6100, validation accuracy 0.923, loss 127.448\n",
      "step 6200, train_loss 55.3516\n",
      "step 6200, validation accuracy 0.9202, loss 127.559\n",
      "step 6300, train_loss 45.5198\n",
      "step 6300, validation accuracy 0.9214, loss 127.586\n",
      "step 6400, train_loss 51.6535\n",
      "step 6400, validation accuracy 0.9188, loss 127.617\n",
      "step 6500, train_loss 52.4212\n",
      "step 6500, validation accuracy 0.9198, loss 127.664\n",
      "step 6600, train_loss 52.4166\n",
      "step 6600, validation accuracy 0.9262, loss 127.655\n",
      "step 6700, train_loss 53.3728\n",
      "step 6700, validation accuracy 0.9206, loss 127.615\n",
      "step 6800, train_loss 52.305\n",
      "step 6800, validation accuracy 0.9202, loss 127.584\n",
      "step 6900, train_loss 51.5773\n",
      "step 6900, validation accuracy 0.919, loss 127.555\n",
      "step 7000, train_loss 48.2743\n",
      "step 7000, validation accuracy 0.9242, loss 127.404\n",
      "step 7100, train_loss 47.7177\n",
      "step 7100, validation accuracy 0.9182, loss 127.318\n",
      "step 7200, train_loss 53.2355\n",
      "step 7200, validation accuracy 0.9162, loss 127.249\n",
      "step 7300, train_loss 50.7359\n",
      "step 7300, validation accuracy 0.9192, loss 127.234\n",
      "step 7400, train_loss 44.3244\n",
      "step 7400, validation accuracy 0.9198, loss 127.128\n",
      "step 7500, train_loss 53.247\n",
      "step 7500, validation accuracy 0.9286, loss 126.993\n",
      "step 7600, train_loss 50.9697\n",
      "step 7600, validation accuracy 0.9214, loss 126.907\n",
      "step 7700, train_loss 55.2223\n",
      "step 7700, validation accuracy 0.9214, loss 126.756\n",
      "step 7800, train_loss 55.9179\n",
      "step 7800, validation accuracy 0.9222, loss 126.59\n",
      "step 7900, train_loss 51.0062\n",
      "step 7900, validation accuracy 0.9172, loss 126.465\n",
      "step 8000, train_loss 51.0216\n",
      "step 8000, validation accuracy 0.9196, loss 126.312\n",
      "step 8100, train_loss 56.869\n",
      "step 8100, validation accuracy 0.9246, loss 126.141\n",
      "step 8200, train_loss 52.8546\n",
      "step 8200, validation accuracy 0.9214, loss 126.006\n",
      "step 8300, train_loss 50.2092\n",
      "step 8300, validation accuracy 0.9188, loss 125.881\n",
      "step 8400, train_loss 56.206\n",
      "step 8400, validation accuracy 0.9182, loss 125.707\n",
      "step 8500, train_loss 44.7754\n",
      "step 8500, validation accuracy 0.92, loss 125.567\n",
      "step 8600, train_loss 51.073\n",
      "step 8600, validation accuracy 0.9218, loss 125.357\n",
      "step 8700, train_loss 51.4491\n",
      "step 8700, validation accuracy 0.921, loss 125.207\n",
      "step 8800, train_loss 49.8148\n",
      "step 8800, validation accuracy 0.9206, loss 125.051\n",
      "step 8900, train_loss 50.8002\n",
      "step 8900, validation accuracy 0.9206, loss 124.864\n",
      "step 9000, train_loss 54.5398\n",
      "step 9000, validation accuracy 0.9208, loss 124.618\n",
      "step 9100, train_loss 52.0044\n",
      "step 9100, validation accuracy 0.9142, loss 124.525\n",
      "step 9200, train_loss 49.0882\n",
      "step 9200, validation accuracy 0.9138, loss 124.382\n",
      "step 9300, train_loss 51.7415\n",
      "step 9300, validation accuracy 0.9222, loss 124.115\n",
      "step 9400, train_loss 54.5603\n",
      "step 9400, validation accuracy 0.9272, loss 123.952\n",
      "step 9500, train_loss 48.4786\n",
      "step 9500, validation accuracy 0.9222, loss 123.801\n",
      "step 9600, train_loss 56.4558\n",
      "step 9600, validation accuracy 0.922, loss 123.603\n",
      "step 9700, train_loss 47.9399\n",
      "step 9700, validation accuracy 0.9228, loss 123.42\n",
      "step 9800, train_loss 46.88\n",
      "step 9800, validation accuracy 0.9188, loss 123.232\n",
      "step 9900, train_loss 50.9539\n",
      "step 9900, validation accuracy 0.9198, loss 122.996\n",
      "step 10000, train_loss 45.1312\n",
      "step 10000, validation accuracy 0.9208, loss 122.81\n",
      "step 10100, train_loss 52.5035\n",
      "step 10100, validation accuracy 0.9174, loss 122.649\n",
      "step 10200, train_loss 54.7212\n",
      "step 10200, validation accuracy 0.9164, loss 122.407\n",
      "step 10300, train_loss 51.2211\n",
      "step 10300, validation accuracy 0.9224, loss 122.213\n",
      "step 10400, train_loss 46.9332\n",
      "step 10400, validation accuracy 0.9162, loss 121.988\n",
      "step 10500, train_loss 45.3961\n",
      "step 10500, validation accuracy 0.9164, loss 121.775\n",
      "step 10600, train_loss 49.1701\n",
      "step 10600, validation accuracy 0.9214, loss 121.582\n",
      "step 10700, train_loss 42.0264\n",
      "step 10700, validation accuracy 0.9246, loss 121.405\n",
      "step 10800, train_loss 43.3188\n",
      "step 10800, validation accuracy 0.9276, loss 121.206\n",
      "step 10900, train_loss 50.4503\n",
      "step 10900, validation accuracy 0.9252, loss 121\n",
      "step 11000, train_loss 52.1785\n",
      "step 11000, validation accuracy 0.9216, loss 120.799\n",
      "step 11100, train_loss 51.7716\n",
      "step 11100, validation accuracy 0.9142, loss 120.61\n",
      "step 11200, train_loss 48.9011\n",
      "step 11200, validation accuracy 0.9162, loss 120.389\n",
      "step 11300, train_loss 47.9043\n",
      "step 11300, validation accuracy 0.9158, loss 120.208\n",
      "step 11400, train_loss 48.4524\n",
      "step 11400, validation accuracy 0.914, loss 120.012\n",
      "step 11500, train_loss 49.5518\n",
      "step 11500, validation accuracy 0.9142, loss 119.825\n",
      "step 11600, train_loss 53.7928\n",
      "step 11600, validation accuracy 0.9178, loss 119.654\n",
      "step 11700, train_loss 51.9675\n",
      "step 11700, validation accuracy 0.9244, loss 119.41\n",
      "step 11800, train_loss 55.777\n",
      "step 11800, validation accuracy 0.9268, loss 119.217\n",
      "step 11900, train_loss 47.4275\n",
      "step 11900, validation accuracy 0.9252, loss 119.006\n",
      "step 12000, train_loss 51.7149\n",
      "step 12000, validation accuracy 0.9288, loss 118.765\n",
      "step 12100, train_loss 52.4912\n",
      "step 12100, validation accuracy 0.9244, loss 118.548\n",
      "step 12200, train_loss 51.5815\n",
      "step 12200, validation accuracy 0.921, loss 118.387\n",
      "step 12300, train_loss 45.1333\n",
      "step 12300, validation accuracy 0.9212, loss 118.197\n",
      "step 12400, train_loss 55.8015\n",
      "step 12400, validation accuracy 0.9164, loss 117.985\n",
      "step 12500, train_loss 48.4878\n",
      "step 12500, validation accuracy 0.92, loss 117.805\n",
      "step 12600, train_loss 49.2737\n",
      "step 12600, validation accuracy 0.9228, loss 117.604\n",
      "step 12700, train_loss 53.7029\n",
      "step 12700, validation accuracy 0.9194, loss 117.393\n",
      "step 12800, train_loss 47.1785\n",
      "step 12800, validation accuracy 0.921, loss 117.174\n",
      "step 12900, train_loss 48.6354\n",
      "step 12900, validation accuracy 0.9162, loss 116.999\n",
      "step 13000, train_loss 45.6746\n",
      "step 13000, validation accuracy 0.9174, loss 116.795\n",
      "step 13100, train_loss 51.3989\n",
      "step 13100, validation accuracy 0.9142, loss 116.583\n",
      "step 13200, train_loss 49.0173\n",
      "step 13200, validation accuracy 0.9188, loss 116.39\n",
      "step 13300, train_loss 57.7228\n",
      "step 13300, validation accuracy 0.9276, loss 116.189\n",
      "step 13400, train_loss 52.3395\n",
      "step 13400, validation accuracy 0.9178, loss 115.98\n",
      "step 13500, train_loss 51.6222\n",
      "step 13500, validation accuracy 0.9248, loss 115.746\n",
      "step 13600, train_loss 55.536\n",
      "step 13600, validation accuracy 0.9204, loss 115.581\n",
      "step 13700, train_loss 53.2042\n",
      "step 13700, validation accuracy 0.9166, loss 115.388\n",
      "step 13800, train_loss 48.7606\n",
      "step 13800, validation accuracy 0.9204, loss 115.195\n",
      "step 13900, train_loss 50.0692\n",
      "step 13900, validation accuracy 0.9206, loss 114.959\n",
      "step 14000, train_loss 56.1339\n",
      "step 14000, validation accuracy 0.922, loss 114.806\n",
      "step 14100, train_loss 66.5289\n",
      "step 14100, validation accuracy 0.914, loss 114.617\n",
      "step 14200, train_loss 48.6976\n",
      "step 14200, validation accuracy 0.9218, loss 114.427\n",
      "step 14300, train_loss 54.4743\n",
      "step 14300, validation accuracy 0.9202, loss 114.22\n",
      "step 14400, train_loss 49.6625\n",
      "step 14400, validation accuracy 0.9156, loss 114.026\n",
      "step 14500, train_loss 48.3493\n",
      "step 14500, validation accuracy 0.9172, loss 113.788\n",
      "step 14600, train_loss 54.0123\n",
      "step 14600, validation accuracy 0.9172, loss 113.618\n",
      "step 14700, train_loss 48.1412\n",
      "step 14700, validation accuracy 0.9186, loss 113.436\n",
      "step 14800, train_loss 48.2649\n",
      "step 14800, validation accuracy 0.9206, loss 113.25\n",
      "step 14900, train_loss 47.651\n",
      "step 14900, validation accuracy 0.9222, loss 113.051\n",
      "test accuracy 0.922\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()    \n",
    "   \n",
    "def relu(passed_mat):\n",
    "    return tf.maximum(0.0, passed_mat)\n",
    "\n",
    "def softmax(passed_mat):\n",
    "    return tf.exp(passed_mat) / tf.reduce_sum(tf.exp(passed_mat))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  batch norm without scale or offset\n",
    "def batch_norm(passed_mat, isTrain, isUncorrupted):\n",
    "    e = 0.00001\n",
    "    decay = 0.999\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    #  track this with moving averages in order to make predictions on the fly\n",
    "    pop_mean = tf.Variable(tf.zeros([passed_mat.get_shape()[-1]]), trainable=False)\n",
    "    pop_var = tf.Variable(tf.ones([passed_mat.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "    \n",
    "    \n",
    "#     if isTrain:\n",
    "#     if Tensor.eval(training):\n",
    "    def training_batch_norn():\n",
    "        mean, var = tf.nn.moments(passed_mat, axes=[0])\n",
    "        #  use moving average to estimate population averages on the fly\n",
    "        \n",
    "        if isUncorrupted:\n",
    "            train_mean = tf.assign(pop_mean, pop_mean * decay + mean * (1 - decay))\n",
    "            train_var = tf.assign(pop_var, pop_var * decay + var * (1 - decay))\n",
    "        else:\n",
    "            train_mean = mean\n",
    "            train_var = var\n",
    "        \n",
    "        #  this makes sure that the pop weights are updated before batch_norm executes\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return _batch_norm_op(passed_mat, mean, var, e)\n",
    "\n",
    "    \n",
    "    def predictive_batch_norm():\n",
    "        #  use population mean and variance instead of batch values\n",
    "        return _batch_norm_op(passed_mat, pop_mean, pop_var, e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return tf.cond(isTrain, training_batch_norn, predictive_batch_norm)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _batch_norm_op(passed_mat, mean, var, e):\n",
    "    #  helps prevent dividing by 0\n",
    "    var_scale = tf.rsqrt(var + e)\n",
    "    bn_out = (passed_mat * var_scale) - (mean * var_scale)\n",
    "    return bn_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  input data and targets\n",
    "x = tf.placeholder(tf.float32, [None, 784], name=\"x\")\n",
    "y_ = tf.placeholder(tf.float32, [None, 10], name=\"y_\")\n",
    "isTrain = tf.placeholder(tf.bool, shape=(), name=\"isTrain\")\n",
    "isLabeled = tf.placeholder(tf.bool, shape=(), name=\"isLabeled\")\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "with tf.name_scope('LadderNetwork') as scope:\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.name_scope('UncorruptedForwardPass'):\n",
    "\n",
    "\n",
    "        #  layer 1\n",
    "        with tf.name_scope('Layer1'):\n",
    "            w1_dim_1, w1_dim_2 = 784, 400\n",
    "            W1 = tf.Variable(tf.random_normal([w1_dim_1, w1_dim_2], stddev=0.001))\n",
    "            b1 = tf.Variable(tf.random_normal([w1_dim_2], stddev=0.001))\n",
    "            z1 = batch_norm(tf.matmul(x, W1) + b1, isTrain, True)\n",
    "            #  these will be trained and adjusted with all other variables\n",
    "            scale1 = tf.Variable(tf.ones([z1.get_shape()[-1]]))\n",
    "            beta1 = tf.Variable(tf.zeros([z1.get_shape()[-1]]))\n",
    "            h1 = relu(scale1 * z1 + beta1)\n",
    "\n",
    "\n",
    "        #  layer 2\n",
    "        with tf.name_scope('Layer2'):\n",
    "            w2_dim_1, w2_dim_2 = 400, 200\n",
    "            W2 = tf.Variable(tf.random_normal([w2_dim_1, w2_dim_2], stddev=0.001))\n",
    "            b2 = tf.Variable(tf.random_normal([w2_dim_2], stddev=0.001))\n",
    "            z2 = batch_norm(tf.matmul(h1, W2) + b2, isTrain, True)\n",
    "            #  these will be trained and adjusted with all other variables\n",
    "            scale2 = tf.Variable(tf.ones([z2.get_shape()[-1]]))\n",
    "            beta2 = tf.Variable(tf.zeros([z2.get_shape()[-1]]))\n",
    "            h2 = relu(scale2 * z2 + beta2)\n",
    "\n",
    "\n",
    "        #  layer 3\n",
    "        with tf.name_scope('Layer3'):\n",
    "            w3_dim_1, w3_dim_2 = 200, 10\n",
    "            W3 = tf.Variable(tf.random_normal([w3_dim_1, w3_dim_2], stddev=0.001))\n",
    "            b3 = tf.Variable(tf.random_normal([w3_dim_2], stddev=0.001))\n",
    "            z3 = batch_norm(tf.matmul(h2, W3) + b3, isTrain, True)\n",
    "            #  these will be trained and adjusted with all other variables\n",
    "            scale3 = tf.Variable(tf.ones([z3.get_shape()[-1]]))\n",
    "            beta3 = tf.Variable(tf.zeros([z3.get_shape()[-1]]))\n",
    "            h3 = softmax(scale3 * z3 + beta3)\n",
    "            y = h3\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.name_scope('CorruptedForwardPass'):\n",
    "\n",
    "        #  _n stands for noisy versions in the corrupted forward pass\n",
    "        true_const_tensor = tf.constant(True)\n",
    "        std = 0.2\n",
    "        \n",
    "        \n",
    "        noise0 = tf.random_normal(shape=tf.shape(x), mean=0.0, stddev=std, dtype=tf.float32) \n",
    "        x_n = h0_n = x + noise0\n",
    "        \n",
    "        \n",
    "        #  layer 1\n",
    "        with tf.name_scope('Layer1'):\n",
    "            z1_n_pre = tf.matmul(h0_n, W1) + b1\n",
    "            noise1 = tf.random_normal(shape=tf.shape(z1_n_pre), mean=0.0, stddev=std, dtype=tf.float32) \n",
    "            z1_n = batch_norm(z1_n_pre, true_const_tensor, False) + noise1\n",
    "            h1_n = relu(scale1 * z1 + beta1)\n",
    "\n",
    "\n",
    "        #  layer 2\n",
    "        with tf.name_scope('Layer2'):\n",
    "            z2_n_pre = tf.matmul(h1_n, W2) + b2\n",
    "            noise2 = tf.random_normal(shape=tf.shape(z2_n_pre), mean=0.0, stddev=std, dtype=tf.float32) \n",
    "            z2_n = batch_norm(z2_n_pre, true_const_tensor, False) + noise2\n",
    "            h2_n = relu(scale2 * z2 + beta2)\n",
    "\n",
    "\n",
    "        #  layer 3\n",
    "        with tf.name_scope('Layer3'):\n",
    "            z3_n_pre = tf.matmul(h2_n, W3) + b3\n",
    "            noise3 = tf.random_normal(shape=tf.shape(z3_n_pre), mean=0.0, stddev=std, dtype=tf.float32) \n",
    "            z3_n = batch_norm(z3_n_pre, true_const_tensor, False) + noise3\n",
    "            h3_n = softmax(scale3 * z3 + beta3)\n",
    "            y_n = h3_n\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    with tf.name_scope('DenoisingBackwardsPass'):\n",
    "\n",
    "\n",
    "        def g_gauss(z_c, u, size):\n",
    "            \"gaussian denoising function proposed in the original paper\"\n",
    "            wi = lambda inits, name: tf.Variable(inits * tf.ones([size]), name=name)\n",
    "            a1 = wi(0., 'a1')\n",
    "            a2 = wi(1., 'a2')\n",
    "            a3 = wi(0., 'a3')\n",
    "            a4 = wi(0., 'a4')\n",
    "            a5 = wi(0., 'a5')\n",
    "\n",
    "            a6 = wi(0., 'a6')\n",
    "            a7 = wi(1., 'a7')\n",
    "            a8 = wi(0., 'a8')\n",
    "            a9 = wi(0., 'a9')\n",
    "            a10 = wi(0., 'a10')\n",
    "\n",
    "            mu = a1 * tf.sigmoid(a2 * u + a3) + a4 * u + a5\n",
    "            v = a6 * tf.sigmoid(a7 * u + a8) + a9 * u + a10\n",
    "\n",
    "            z_est = (z_c - mu) * v + mu\n",
    "            return z_est\n",
    "\n",
    "\n",
    "\n",
    "        d_cost = []  # to store the denoising cost of all layers\n",
    "\n",
    "        #  layer 3\n",
    "        with tf.name_scope('Layer3'):\n",
    "            \n",
    "            #  this first layer doesn not downsample but measures the loss of of output as compared\n",
    "            #  to it's noisey version and it's batch normed counter part\n",
    "            z = z3\n",
    "            z_c = z3_n\n",
    "\n",
    "            u = y_n\n",
    "            #  TODO:  put u_bn back to batch norm\n",
    "            u_bn = y_n\n",
    "#             u_bn = batch_norm(u, isTrain, False)\n",
    "            z3_dn = g_gauss(z_c, u_bn, w3_dim_2)\n",
    "            z3_dn_bn = batch_norm(z3_dn, isTrain, False)\n",
    "    \n",
    "            # append the cost of this layer to d_cost\n",
    "            d_cost.append((tf.reduce_mean(tf.reduce_sum(tf.square(z - z3_dn_bn), 1)) / w3_dim_2) * 0.10) \n",
    "    \n",
    "    \n",
    "\n",
    "        #  layer 2\n",
    "        with tf.name_scope('Layer2'):\n",
    "            \n",
    "            V2 = tf.Variable(tf.random_normal([w3_dim_2, w3_dim_1], stddev=0.001))\n",
    "            u_bn = batch_norm(tf.matmul(z3_dn, V2), isTrain, False)  \n",
    "            \n",
    "            z = z2\n",
    "            z_c = z2_n\n",
    "            z2_dn = g_gauss(z_c, u_bn, w2_dim_2)\n",
    "            z2_dn_bn = batch_norm(z2_dn, isTrain, False)\n",
    "\n",
    "            # append the cost of this layer to d_cost\n",
    "            d_cost.append((tf.reduce_mean(tf.reduce_sum(tf.square(z - z2_dn_bn), 1)) / w2_dim_2) * 0.10)\n",
    "           \n",
    "              \n",
    "        \n",
    "        #  layer 1\n",
    "        with tf.name_scope('Layer1'):\n",
    "                \n",
    "            V1 = tf.Variable(tf.random_normal([w2_dim_2, w2_dim_1], stddev=0.001))\n",
    "            u_bn = batch_norm(tf.matmul(z2_dn, V1), isTrain, False) \n",
    "                       \n",
    "            z = z1\n",
    "            z_c = z1_n\n",
    "            z1_dn = g_gauss(z_c, u_bn, w1_dim_2)\n",
    "            z1_dn_bn = batch_norm(z1_dn, isTrain, False)  #  batch norm the denoised z for the d_cost\n",
    "\n",
    "            # append the cost of this layer to d_cost\n",
    "            d_cost.append((tf.reduce_mean(tf.reduce_sum(tf.square(z - z1_dn_bn), 1)) / w1_dim_2) * 10.0)\n",
    "  \n",
    "            \n",
    "                   \n",
    "        #  layer 0\n",
    "        with tf.name_scope('Layer0'): \n",
    "                \n",
    "            V0 = tf.Variable(tf.random_normal([w1_dim_2, w1_dim_1], stddev=0.001))\n",
    "            u_bn = batch_norm(tf.matmul(z1_dn, V0), isTrain, False) \n",
    "                       \n",
    "            z = x\n",
    "            z_c = x_n\n",
    "            z0_dn = g_gauss(z_c, u_bn, w1_dim_1)\n",
    "            z0_dn_bn = batch_norm(z0_dn, isTrain, False)\n",
    "\n",
    "            # append the cost of this layer to d_cost\n",
    "            d_cost.append((tf.reduce_mean(tf.reduce_sum(tf.square(z - z0_dn_bn), 1)) / w1_dim_1) * 1000.0)\n",
    "\n",
    "        x_dn = z0_dn\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope('SupervisedLoss') as scope:\n",
    "    def cross_ent():\n",
    "        return tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_n), reduction_indices=[1]))\n",
    "    def set_to_zero():\n",
    "        return tf.constant(0.0)\n",
    "    cross_entropy = tf.cond(isLabeled, cross_ent, set_to_zero)\n",
    "#     cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_n), reduction_indices=[1]))\n",
    "\n",
    "    \n",
    "\n",
    "with tf.name_scope('UnupervisedLoss') as scope:\n",
    "    # calculate total unsupervised cost by adding the denoising cost of all layers\n",
    "    unsupervised_cost = tf.add_n(d_cost)\n",
    "    loss = cross_entropy + unsupervised_cost  # total cost\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "with tf.name_scope('Optimizer') as scope:\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "#     train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    \n",
    "with tf.name_scope('Accuracy') as scope:\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#  summary writers for debugging\n",
    "summary_writer = tf.summary.FileWriter( \\\n",
    "        '/home/tuna/Projects/CS678/Ladder_Networks_Project_1/tf_logs/ln_mnist_5000_labels/v6', graph=sess.graph )\n",
    "acc_summary = tf.summary.scalar( 'accuracy', accuracy )\n",
    "loss_summary = tf.summary.scalar( 'loss', loss )\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "#  additional for seperate runs\n",
    "valid_acc_summary = tf.summary.scalar( 'validation_accuracy', accuracy )\n",
    "test_acc_summary = tf.summary.scalar( 'test_accuracy', accuracy )\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "\n",
    "for i in range(15000):\n",
    "#     batch = mnist.train.next_batch(batch_size)\n",
    "    batch_xs, batch_ys = get_next_batch(mnist.train.images, mnist_train_y_5000_labeled, batch_size)\n",
    "\n",
    "#     if i < 1000:\n",
    "#         _, summary_, train_loss = sess.run([train_step, merged_summary_op, loss], feed_dict={x: batch_xs, y_: batch_ys,\n",
    "#                                                                           isTrain: True, isLabeled: True})\n",
    "#     else:\n",
    "#         _, summary_, train_loss = sess.run([train_step, merged_summary_op, loss], feed_dict={x: batch_xs, y_: np.zeros(batch_ys.shape, dtype=np.float32),\n",
    "#                                                                           isTrain: True, isLabeled: True})\n",
    "#     _, summary_ = sess.run([train_step, merged_summary_op], feed_dict={x: batch[0], y_: np.zeros(batch[1].shape, dtype=np.float32),\n",
    "#                                                                       isTrain: True, isLabeled: False})\n",
    "    _, summary_, train_loss = sess.run([train_step, merged_summary_op, loss], feed_dict={x: batch_xs, y_: batch_ys,\n",
    "                                                                      isTrain: True, isLabeled: True})\n",
    "    \n",
    "\n",
    "    summary_writer.add_summary(summary_, i)\n",
    "    if i%100 == 0:\n",
    "        #  print training loss\n",
    "        print(\"step %d, train_loss %g\" % (i, train_loss))\n",
    "        \n",
    "        #  validation set results\n",
    "        acc, ls, valid_summary = sess.run([accuracy, loss, valid_acc_summary], \\\n",
    "                                        feed_dict={x: mnist.validation.images, y_: mnist.validation.labels,\\\n",
    "                                                  isTrain: False, isLabeled: True})\n",
    "        summary_writer.add_summary(valid_summary, i)\n",
    "        print(\"step %d, validation accuracy %g, loss %g\" % (i, acc, ls))\n",
    "\n",
    "        \n",
    "#  test set results\n",
    "acc, test_acc_summary = sess.run([accuracy, test_acc_summary], feed_dict={x: mnist.test.images, y_: mnist.test.labels,\\\n",
    "                                                          isTrain: False, isLabeled: True})\n",
    "summary_writer.add_summary(test_acc_summary, 1)\n",
    "summary_writer.add_summary(test_acc_summary, 100)\n",
    "summary_writer.add_summary(test_acc_summary, 1000)\n",
    "summary_writer.add_summary(test_acc_summary, 10)\n",
    "print(\"test accuracy %g\" % acc)\n",
    "\n",
    "\n",
    "\n",
    "summary_writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # this is a simpler version of Tensorflow's 'official' version. See:\n",
    "# # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L102\n",
    "# def batch_norm_wrapper(inputs, isTrain, decay = 0.999):\n",
    "\n",
    "#     scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "#     beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "#     pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "#     pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "    \n",
    "#     epsilon = 0.00001\n",
    "    \n",
    "    \n",
    "#     def training_batch_norm():\n",
    "#         batch_mean, batch_var = tf.nn.moments(inputs,[0])\n",
    "#         train_mean = tf.assign(pop_mean,\n",
    "#                                pop_mean * decay + batch_mean * (1 - decay))\n",
    "#         train_var = tf.assign(pop_var,\n",
    "#                               pop_var * decay + batch_var * (1 - decay))\n",
    "#         with tf.control_dependencies([train_mean, train_var]):\n",
    "#             return batch_norm_op(inputs,\n",
    "#                 batch_mean, batch_var, beta, scale, epsilon)\n",
    "    \n",
    "#     def predictive_batch_norm():\n",
    "#         return batch_norm_op(inputs,\n",
    "#             pop_mean, pop_var, beta, scale, epsilon)\n",
    "\n",
    "#     return tf.cond(isTrain, training_batch_norm, predictive_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_normalization(x,\n",
    "                        mean,\n",
    "                        variance,\n",
    "                        offset,\n",
    "                        scale,\n",
    "                        variance_epsilon,\n",
    "                        name=None):\n",
    "  r\"\"\"Batch normalization.\n",
    "  As described in http://arxiv.org/abs/1502.03167.\n",
    "  Normalizes a tensor by `mean` and `variance`, and applies (optionally) a\n",
    "  `scale` \\\\(\\gamma\\\\) to it, as well as an `offset` \\\\(\\beta\\\\):\n",
    "  \\\\(\\frac{\\gamma(x-\\mu)}{\\sigma}+\\beta\\\\)\n",
    "  `mean`, `variance`, `offset` and `scale` are all expected to be of one of two\n",
    "  shapes:\n",
    "    * In all generality, they can have the same number of dimensions as the\n",
    "      input `x`, with identical sizes as `x` for the dimensions that are not\n",
    "      normalized over (the 'depth' dimension(s)), and dimension 1 for the\n",
    "      others which are being normalized over.\n",
    "      `mean` and `variance` in this case would typically be the outputs of\n",
    "      `tf.nn.moments(..., keep_dims=True)` during training, or running averages\n",
    "      thereof during inference.\n",
    "    * In the common case where the 'depth' dimension is the last dimension in\n",
    "      the input tensor `x`, they may be one dimensional tensors of the same\n",
    "      size as the 'depth' dimension.\n",
    "      This is the case for example for the common `[batch, depth]` layout of\n",
    "      fully-connected layers, and `[batch, height, width, depth]` for\n",
    "      convolutions.\n",
    "      `mean` and `variance` in this case would typically be the outputs of\n",
    "      `tf.nn.moments(..., keep_dims=False)` during training, or running averages\n",
    "      thereof during inference.\n",
    "  Args:\n",
    "    x: Input `Tensor` of arbitrary dimensionality.\n",
    "    mean: A mean `Tensor`.\n",
    "    variance: A variance `Tensor`.\n",
    "    offset: An offset `Tensor`, often denoted \\\\(\\beta\\\\) in equations, or\n",
    "      None. If present, will be added to the normalized tensor.\n",
    "    scale: A scale `Tensor`, often denoted \\\\(\\gamma\\\\) in equations, or\n",
    "      `None`. If present, the scale is applied to the normalized tensor.\n",
    "    variance_epsilon: A small float number to avoid dividing by 0.\n",
    "    name: A name for this operation (optional).\n",
    "  Returns:\n",
    "    the normalized, scaled, offset tensor.\n",
    "  \"\"\"\n",
    "  with ops.name_scope(name, \"batchnorm\", [x, mean, variance, scale, offset]):\n",
    "    inv = math_ops.rsqrt(variance + variance_epsilon)\n",
    "    if scale is not None:\n",
    "      inv *= scale\n",
    "    return x * inv + (offset - mean * inv\n",
    "                      if offset is not None else -mean * inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     with tf.name_scope('DenoisingBackwardsPass'):\n",
    "\n",
    "\n",
    "            \n",
    "#         #  TODO: I think I'm missing something big here... each z_n is only dependant on it's\n",
    "#         #  realted z_n_i? I'm just using the entire layer on both sides... they say I should\n",
    "#         #  seperate these things and make them independant?\n",
    "        \n",
    "        \n",
    "\n",
    "#         #  layer 3\n",
    "#         with tf.name_scope('Layer3'):\n",
    "            \n",
    "            \n",
    "#             V3 = tf.Variable(tf.random_normal([w3_dim_2, w3_dim_1], stddev=0.001))\n",
    "# #             u3 = batch_norm(V3 * z4_dn)  #  I'll prob need to change this to y in the future\n",
    "#             u3 = y_n\n",
    "\n",
    "            \n",
    "#             a3_1 = tf.Variable(tf.random_normal([batch_size, w3_dim_2], stddev=0.001))\n",
    "#             a3_2 = tf.Variable(tf.random_normal(tf.shape(u3), stddev=0.001), validate_shape=False)\n",
    "#             a3_3 = tf.Variable(tf.random_normal(tf.shape(u3), stddev=0.001), validate_shape=False)\n",
    "#             a3_4 = tf.Variable(tf.random_normal(tf.shape(u3), stddev=0.001), validate_shape=False)\n",
    "#             a3_5 = tf.Variable(tf.random_normal(tf.shape(u3), stddev=0.001), validate_shape=False)\n",
    "#             mu_3 = a3_1 * tf.sigmoid(a3_2 * u3 + a3_3) + a3_4 * u3 + a3_5\n",
    "                        \n",
    "#             a3_6 = tf.Variable(tf.random_normal(tf.shape(u3), stddev=0.001), validate_shape=False)\n",
    "#             a3_7 = tf.Variable(tf.random_normal(tf.shape(u3), stddev=0.001), validate_shape=False)\n",
    "#             a3_8 = tf.Variable(tf.random_normal(tf.shape(u3), stddev=0.001), validate_shape=False)\n",
    "#             a3_9 = tf.Variable(tf.random_normal(tf.shape(u3), stddev=0.001), validate_shape=False)\n",
    "#             a3_10 = tf.Variable(tf.random_normal(tf.shape(u3), stddev=0.001), validate_shape=False)\n",
    "#             v_3 = a3_6 * tf.sigmoid(a3_7 * u3 + a3_8) + a3_9 * u3 + a3_10\n",
    "\n",
    "#             z3_dn = (z3_n - mu_3) * v_3 + mu_3\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#         #  layer 2\n",
    "#         with tf.name_scope('Layer2'):\n",
    "            \n",
    "             \n",
    "#             V2 = tf.Variable(tf.random_normal([w2_dim_2, w2_dim_1], stddev=0.001))\n",
    "#             u2 = batch_norm(V2 * z3_dn, isTrain, True)  #  I'll prob need to change this to y in the future\n",
    "\n",
    "            \n",
    "#             a2_1 = tf.Variable(tf.random_normal(tf.shape(u2), stddev=0.001), validate_shape=False)\n",
    "#             a2_2 = tf.Variable(tf.random_normal(tf.shape(u2), stddev=0.001), validate_shape=False)\n",
    "#             a2_3 = tf.Variable(tf.random_normal(tf.shape(u2), stddev=0.001), validate_shape=False)\n",
    "#             a2_4 = tf.Variable(tf.random_normal(tf.shape(u2), stddev=0.001), validate_shape=False)\n",
    "#             a2_5 = tf.Variable(tf.random_normal(tf.shape(u2), stddev=0.001), validate_shape=False)\n",
    "#             mu_2 = a2_1 * tf.sigmoid(a2_2 * u2 + a2_3) + a2_4 * u2 + a2_5\n",
    "                        \n",
    "#             a2_6 = tf.Variable(tf.random_normal(tf.shape(u2), stddev=0.001), validate_shape=False)\n",
    "#             a2_7 = tf.Variable(tf.random_normal(tf.shape(u2), stddev=0.001), validate_shape=False)\n",
    "#             a2_8 = tf.Variable(tf.random_normal(tf.shape(u2), stddev=0.001), validate_shape=False)\n",
    "#             a2_9 = tf.Variable(tf.random_normal(tf.shape(u2), stddev=0.001), validate_shape=False)\n",
    "#             a2_10 = tf.Variable(tf.random_normal(tf.shape(u2), stddev=0.001), validate_shape=False)\n",
    "#             v_2 = a2_6 * tf.sigmoid(a2_7 * u2 + a2_8) + a2_9 * u2 + a2_10\n",
    "\n",
    "#             z2_dn = (z2_n - mu_2) * v_2 + mu_2\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "           \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "#         #  layer 1\n",
    "#         with tf.name_scope('Layer1'):\n",
    "                \n",
    "                \n",
    "#             V1 = tf.Variable(tf.random_normal([W1.get_shape()[1], W1.get_shape()[0]], stddev=0.001))\n",
    "#             u1 = batch_norm(V1 * z1_n)  #  I'll prob need to change this to y in the future\n",
    "\n",
    "            \n",
    "#             a1_1 = tf.Variable(tf.random_normal([u1.get_shape()], stddev=0.001))\n",
    "#             a1_2 = tf.Variable(tf.random_normal([u1.get_shape()], stddev=0.001))\n",
    "#             a1_3 = tf.Variable(tf.random_normal([u1.get_shape()], stddev=0.001))\n",
    "#             a1_4 = tf.Variable(tf.random_normal([u1.get_shape()], stddev=0.001))\n",
    "#             a1_5 = tf.Variable(tf.random_normal([u1.get_shape()], stddev=0.001))\n",
    "#             mu_1 = a1_1 * tf.sigmoid(a1_2 * u1 + a1_3) + a1_4 * u1 + a1_5\n",
    "                        \n",
    "#             a1_6 = tf.Variable(tf.random_normal([u1.get_shape()], stddev=0.001))\n",
    "#             a1_7 = tf.Variable(tf.random_normal([u1.get_shape()], stddev=0.001))\n",
    "#             a1_8 = tf.Variable(tf.random_normal([u1.get_shape()], stddev=0.001))\n",
    "#             a1_9 = tf.Variable(tf.random_normal([u1.get_shape()], stddev=0.001))\n",
    "#             a1_10 = tf.Variable(tf.random_normal([u1.get_shape()], stddev=0.001))\n",
    "#             v_1 = a1_6 * tf.sigmoid(a1_7 * u1 + a1_8) + a1_9 * u1 + a1_10\n",
    "\n",
    "#             z1_dn = (z1_n - mu_1) * v_1 + mu_1\n",
    "    \n",
    "#             #  TODO might be incorrect, are there more transformations to get from\n",
    "#             #  z1 to z0 to x_dn ?\n",
    "#             x_dn = z1_dn\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# print \"=== Decoder ===\"\n",
    "\n",
    "\n",
    "# def g_gauss(z_c, u, size):\n",
    "#     \"gaussian denoising function proposed in the original paper\"\n",
    "#     wi = lambda inits, name: tf.Variable(inits * tf.ones([size]), name=name)\n",
    "#     a1 = wi(0., 'a1')\n",
    "#     a2 = wi(1., 'a2')\n",
    "#     a3 = wi(0., 'a3')\n",
    "#     a4 = wi(0., 'a4')\n",
    "#     a5 = wi(0., 'a5')\n",
    "\n",
    "#     a6 = wi(0., 'a6')\n",
    "#     a7 = wi(1., 'a7')\n",
    "#     a8 = wi(0., 'a8')\n",
    "#     a9 = wi(0., 'a9')\n",
    "#     a10 = wi(0., 'a10')\n",
    "\n",
    "#     mu = a1 * tf.sigmoid(a2 * u + a3) + a4 * u + a5\n",
    "#     v = a6 * tf.sigmoid(a7 * u + a8) + a9 * u + a10\n",
    "\n",
    "#     z_est = (z_c - mu) * v + mu\n",
    "#     return z_est\n",
    "\n",
    "# # Decoder\n",
    "# z_est = {}\n",
    "# d_cost = []  # to store the denoising cost of all layers\n",
    "# for l in range(L, -1, -1):\n",
    "#     print \"Layer \", l, \": \", layer_sizes[l+1] if l+1 < len(layer_sizes) else None, \" -> \", layer_sizes[l], \", denoising cost: \", denoising_cost[l]\n",
    "#     z, z_c = clean['unlabeled']['z'][l], corr['unlabeled']['z'][l]\n",
    "#     m, v = clean['unlabeled']['m'].get(l, 0), clean['unlabeled']['v'].get(l, 1-1e-10)\n",
    "#     if l == L:\n",
    "#         u = unlabeled(y_c)\n",
    "#     else:\n",
    "#         u = tf.matmul(z_est[l+1], weights['V'][l])\n",
    "#     u = batch_normalization(u)\n",
    "#     z_est[l] = g_gauss(z_c, u, layer_sizes[l])\n",
    "#     z_est_bn = (z_est[l] - m) / v\n",
    "#     # append the cost of this layer to d_cost\n",
    "#     d_cost.append((tf.reduce_mean(tf.reduce_sum(tf.square(z_est_bn - z), 1)) / layer_sizes[l]) * denoising_cost[l])\n",
    "# print \"=== Decoder ===\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
